{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATE Algorithm Evaluation with Synthetic Data\n",
    "\n",
    "This notebook \n",
    "\n",
    "1. shows how to use EBM and econml to generate EBM CATE estimators. \n",
    "2. setup a complete performance evaluation/comparison pipeline for any CATE estimators. \n",
    "\n",
    "\n",
    "[To be added: some visualization]\n",
    "\n",
    "\n",
    "\n",
    "### DGP output\n",
    "```\n",
    "data = {'X':, 'P':,'T':, 'Y':,'Y0':, 'Y1':, \n",
    "        'treatment_effect':, 'X_test':, 'treatment_effect_test':,'Y_test':,'special_test':}\n",
    "```\n",
    "The generated data is formated as valid input of BaseCateEstimator.fit() in the econml package:\n",
    "```\n",
    "    Y: (n × d_y) matrix of outcomes for each sample\n",
    "    T: (n × d_t) matrix of treatments for each sample\n",
    "    X: optional (n × d_x) matrix of features for each sample\n",
    "    W: optional (n × d_w) matrix of controls for each sample\n",
    "    Z: optional (n × d_z) matrix of instruments for each sample\n",
    "    inference: optional string, `Inference` instance, or None\n",
    "        Method for performing inference.  All estimators support 'bootstrap'\n",
    "        (or an instance of `BootstrapInference`), some support other methods as well.\n",
    "```\n",
    "\n",
    "### \n",
    "\n",
    "### Evaluation Criteria\n",
    "MSE of tau: $MSE(\\tau, \\hat{\\tau}) = \\frac{1}{n}\\sum_{i=1}^n (\\hat{\\tau}(x_i) - \\tau_i)^2$\n",
    "\n",
    "## util functions\n",
    "- `data_generator`: a general DGP generator \n",
    "- `regressor`: an automl regressor based on GridSearchCV\n",
    "- `clf`:  an automl classifier based on GridSearchCV\n",
    "- `generate_EBM_CATE_learners`: generate a dictionary of EBM CATE learners\n",
    "- `generate_CATE_learners`: generate a dictionary of CATE learners\n",
    "- `run_and_report_mse`: report mse of estimating tau for a list of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CATE_utils import data_generator, regressor, clf, generate_EBM_CATE_learners, generate_CATE_learners, run_and_report_mse\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import time\n",
    "from econml.dr import DRLearner, LinearDRLearner, SparseLinearDRLearner, ForestDRLearner\n",
    "from econml.metalearners import TLearner, SLearner, XLearner, DomainAdaptationLearner\n",
    "from econml.orf import DROrthoForest\n",
    "from econml.dml import DML, LinearDML, SparseLinearDML, CausalForestDML, NonParamDML\n",
    "from econml.sklearn_extensions.model_selection import GridSearchCVList\n",
    "from econml.sklearn_extensions.linear_model import WeightedLassoCVWrapper, WeightedLasso, WeightedLassoCV\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, Lasso, LogisticRegression\n",
    "from econml.sklearn_extensions.linear_model import WeightedLassoCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, RandomForestRegressor, RandomForestClassifier\n",
    "from interpret.glassbox import ExplainableBoostingRegressor, ExplainableBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from interpret import show\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGP 1 \n",
    "(From 'Doubly Robust Learner an Interpretability.ipynb')\n",
    "* Binary covariates (segments naturally defined)\n",
    "* $X_{i1}$ determines both $p$ and $\\tau$, $X_{i1}$ is the only confounder and generates heterogeneity\n",
    "* Heteroskedastic error in $X_{i1}$\n",
    "* $X_{i-1}$ are irrelevant for $p$ or $\\tau$ or heteroskedasticity\n",
    "\n",
    "\\begin{align}\n",
    "X_i & = (X_{i1}, X_{i2}, X_{i3}, X_{i4}) \\overset{iid}{\\sim} \\operatorname{Ber}(0.5)\\\\\n",
    "p_i & = \\operatorname{expit}(X_{i1}) = \\left\\{\n",
    "\\begin{array}{cc}\n",
    "0.5 & X_{i1} = 0\\\\\n",
    "0.73 & X_{i1} = 1\n",
    "\\end{array}\\right.\\\\\n",
    "Y_i(0) & = X_{i1} + (X_{i1} + 1)*\\operatorname{N}(0, 1)\\\\\n",
    "\\tau_i & = -1 + 2 X_{i1}\n",
    "\\end{align}\n",
    "\n",
    "## DGP 2 \n",
    "(Frome Metalearner Examples.ipynb)\n",
    "We use the data generating process (DGP) from [Kunzel et al.](https://arxiv.org/abs/1706.03461). The DGP is described by the following equations:\n",
    "\n",
    "\\begin{align}\n",
    "X_i & \\sim N(0, I)\\\\\n",
    "p_i & =  \\left\\{\n",
    "\\begin{array}{cc}\n",
    "0.8 & X_{i3} \\in [-0.5, 0.5]\\\\\n",
    "0.2 & X_{i3} \\notin [-0.5, 0.5]\n",
    "\\end{array}\\right.\\\\\n",
    "Y_i(0) & = X_i^T\\beta + \\operatorname{N}(0, 1)\\\\\n",
    "\\tau_i & = 8 \\mathbb{I}(x_2>0.1)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "## DGP3\n",
    "We use the Response Surface B from [Hill (2011)](https://www.tandfonline.com/doi/pdf/10.1198/jcgs.2010.08162) to generate sythetic outcome surfaces from real-world covariates and treatment assignments (Infant Health Development Program data). Since the original data was part of a randomized trial, a subset of the treated infants (those with non-white mothers) has been removed from the data in order to mimic the observational data setting. For more details, see [Hill (2011)](https://www.tandfonline.com/doi/pdf/10.1198/jcgs.2010.08162).\n",
    "\n",
    "\n",
    "The DGP is described by the following equations:\n",
    "\n",
    "$\n",
    "Y(0) = e^{(X+W)\\beta} + \\epsilon_0, \\;\\epsilon_0 \\sim N(0, 1)\\\\\n",
    "Y(1) = X\\beta - \\omega + \\epsilon_1, \\;\\epsilon_1 \\sim N(0, 1)\\\\\n",
    "$\n",
    "\n",
    "where $X$ is a covariate matrix, $W$ is a constant matrix with entries equal to $0.5$ and $w$ is a constant calculated such that the CATT equals $4$.\n",
    "\n",
    "\n",
    "## DGP4 \n",
    "(From 'Double Machine Learning Examples.ipynb')\n",
    "We use the data generating process (DGP) from [Oprescu 2019](https://arxiv.org/abs/1806.03467). The DGP is described by the following equations:\n",
    "\n",
    "\\begin{align}\n",
    "X_i \\sim& \\text{Uniform}(0,1)\\\\\n",
    "W_i \\sim& \\text{Normal}(0,\\, I_{30})\\\\\n",
    "p_i =& expit(W_i^T\\beta)\\\\\n",
    "Y_i(0) =& \\langle W, \\gamma\\rangle + \\epsilon, &\\; \\epsilon \\sim \\text{Uniform}(-1, 1)\\\\\n",
    "\\tau_i =& \\exp(2\\cdot x_1) \n",
    "\\end{align}\n",
    "\n",
    "where $W$ is a matrix of high-dimensional confounders and $\\beta, \\gamma$ have high sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Treatment effect function\n",
    "# def exp_te(x):\n",
    "#     return np.exp(2 * x[0])# DGP constants\n",
    "\n",
    "# np.random.seed(123)\n",
    "# n = 1000\n",
    "# n_w = 30\n",
    "# support_size = 5\n",
    "# n_x = 4\n",
    "# # Outcome support\n",
    "# support_Y = np.random.choice(range(n_w), size=support_size, replace=False)\n",
    "# coefs_Y = np.random.uniform(0, 1, size=support_size)\n",
    "# epsilon_sample = lambda n:np.random.uniform(-1, 1, size=n)\n",
    "# # Treatment support\n",
    "# support_T = support_Y\n",
    "# coefs_T = np.random.uniform(0, 1, size=support_size)\n",
    "# eta_sample = lambda n: np.random.uniform(-1, 1, size=n) \n",
    "\n",
    "# # Generate controls, covariates, treatments and outcomes\n",
    "# W = np.random.normal(0, 1, size=(n, n_w))\n",
    "# X = np.random.uniform(0, 1, size=(n, n_x))\n",
    "# # Heterogeneous treatment effects\n",
    "# TE = np.array([exp_te(x_i) for x_i in X])\n",
    "# # Define treatment\n",
    "# log_odds = np.dot(W[:, support_T], coefs_T) + eta_sample(n)\n",
    "# T_sigmoid = 1/(1 + np.exp(-log_odds))\n",
    "# T = np.array([np.random.binomial(1, p) for p in T_sigmoid])\n",
    "# # Define the outcome\n",
    "# Y = TE * T + np.dot(W[:, support_Y], coefs_Y) + epsilon_sample(n)\n",
    "\n",
    "# # get testing data\n",
    "# X_test = np.random.uniform(0, 1, size=(n, n_x))\n",
    "# X_test[:, 0] = np.linspace(0, 1, n)\n",
    "# expected_te = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DGP3\n",
    "# n = 1000\n",
    "# d = 5\n",
    "# covariates_model = lambda d: multivariate_normal(np.zeros(d), np.diag(np.ones(d)), 1)\n",
    "# propensity_model = lambda x: (0.8 if (x[2]>-0.5 and x[2]<0.5) else 0.2)\n",
    "# beta = np.random.uniform(-3, 3, d)\n",
    "# control_outcome_model = lambda x: np.dot(x, beta) + np.random.normal(0, 1)\n",
    "# treatment_effect_model = lambda x: (1 if x[1] > 0.1 else 0)*8\n",
    "# data2 = data_generator(n=1000, \n",
    "#                       d=4, \n",
    "#                       covariates_model=covariates_model,\n",
    "#                       propensity_model=propensity_model,\n",
    "#                       control_outcome_model=control_outcome_model,\n",
    "#                       treatment_effect_model=treatment_effect_model,\n",
    "#                       seed = 1234\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DGP2\n",
    "# n = 1000\n",
    "# d = 5\n",
    "# covariates_model = lambda d: multivariate_normal(np.zeros(d), np.diag(np.ones(d)), 1)\n",
    "# propensity_model = lambda x: (0.8 if (x[2]>-0.5 and x[2]<0.5) else 0.2)\n",
    "# beta = np.random.uniform(-3, 3, d)\n",
    "# control_outcome_model = lambda x: np.dot(x, beta) + np.random.normal(0, 1)\n",
    "# treatment_effect_model = lambda x: (1 if x[1] > 0.1 else 0)*8\n",
    "# data2 = data_generator(n=1000, \n",
    "#                       d=4, \n",
    "#                       covariates_model=covariates_model,\n",
    "#                       propensity_model=propensity_model,\n",
    "#                       control_outcome_model=control_outcome_model,\n",
    "#                       treatment_effect_model=treatment_effect_model,\n",
    "#                       seed = 1234\n",
    "#                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP1\n",
    "n = 200\n",
    "d = 4\n",
    "covariates_model = lambda d: np.random.binomial(1, .5, size=d)\n",
    "propensity_model = lambda x: scipy.special.expit(x[0])\n",
    "control_outcome_model = lambda x: x[0] + (x[0] + 1)*np.random.normal(0, 1, size=1)\n",
    "treatment_effect_model = lambda x: -1 + 2 * x[0]\n",
    "special_test_point = np.array([[1, 0, 0, 0], [0, 0, 0, 0]])\n",
    "special_test_value = np.array([1, -1])\n",
    "\n",
    "\n",
    "data1 = data_generator(n=1000, \n",
    "                      d=4, \n",
    "                      covariates_model=covariates_model,\n",
    "                      propensity_model=propensity_model,\n",
    "                      control_outcome_model=control_outcome_model,\n",
    "                      treatment_effect_model=treatment_effect_model,\n",
    "                      special_test_point=special_test_point,\n",
    "                      special_test_value=special_test_value,\n",
    "                      seed = 1234\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data1\n",
    "X = data['X']\n",
    "T = data['T']\n",
    "Y = data['Y']\n",
    "X_test = data['X_test']\n",
    "treatment_effect_test = data['treatment_effect_test']\n",
    "Y_test = data['Y_test']\n",
    "# P = data['P']\n",
    "# Y0 = data['Y0']\n",
    "# Y1 = data['Y1']\n",
    "# treatment_effect = data['treatment_effect']\n",
    "# special_test_point, special_test_value = data['special_test']\n",
    "\n",
    "model_y = clone(regressor().fit(X, Y).best_estimator_)\n",
    "model_t = clone(clf().fit(X, T).best_estimator_)\n",
    "\n",
    "n = X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<econml.dml.dml.NonParamDML at 0x7fee8a9c3690>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed =100\n",
    "est_NonParamDML = NonParamDML(model_y=ExplainableBoostingRegressor(random_state=seed, outer_bags=20),\n",
    "                                model_t=model_t,\n",
    "                                model_final=ExplainableBoostingRegressor(random_state=seed, outer_bags=20),\n",
    "                                discrete_treatment=True,\n",
    "                                random_state=seed)\n",
    "est_NonParamDML.fit(Y, T, X=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7724/140662504437136/ -->\n",
       "<iframe src=\"http://127.0.0.1:7724/140662504437136/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(est_NonParamDML.model_cate.explain_global())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tlearner\n",
      "NonParamDML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [nan]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'sample_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-baa519665e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mebm_learner_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_EBM_CATE_learners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_and_report_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tlearner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NonParamDML'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment_effect_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m+\u001b[0m \u001b[0mrun_and_report_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EBM_Tlearner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EBM_NonParamDML'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebm_learner_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment_effect_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# records = run_and_report_mse(learner_dic.keys(), learner_dic, Y, T, X, X_test, treatment_effect_test) + run_and_report_mse(ebm_learner_dic.keys(), ebm_learner_dic, Y, T, X, X_test, treatment_effect_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oneproject/One%20Project/Research/CausalEBM/CATE_utils.py\u001b[0m in \u001b[0;36mrun_and_report_mse\u001b[0;34m(model_list, learner_dic, Y, T, X, X_test, treatment_effect_test)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fit_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0merror_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtreatment_effect_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/utilities.py\u001b[0m in \u001b[0;36mm\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrong_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mto_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/dml/dml.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, Y, T, X, W, sample_weight, freq_weight, sample_var, groups, cache_values, inference)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                            \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                            \u001b[0mcache_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m                            inference=inference)\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrefit_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/utilities.py\u001b[0m in \u001b[0;36mm\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrong_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mto_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/dml/_rlearner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, Y, T, X, W, sample_weight, freq_weight, sample_var, groups, cache_values, inference)\u001b[0m\n\u001b[1;32m    370\u001b[0m                            \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                            \u001b[0mcache_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                            inference=inference)\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/utilities.py\u001b[0m in \u001b[0;36mm\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrong_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mto_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/_cate_estimator.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, Y, T, inference, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# call the wrapped fit method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_postfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minference\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/_ortho_learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, Y, T, X, W, Z, sample_weight, freq_weight, sample_var, groups, cache_values, inference, only_final, check_input)\u001b[0m\n\u001b[1;32m    686\u001b[0m                         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                         \u001b[0mfreq_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m                         sample_var=sample_var)\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/_ortho_learner.py\u001b[0m in \u001b[0;36m_fit_final\u001b[0;34m(self, Y, T, X, W, Z, nuisances, sample_weight, freq_weight, sample_var)\u001b[0m\n\u001b[1;32m    776\u001b[0m                                                                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                                                                        \u001b[0mfreq_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m                                                                        sample_var=sample_var))\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ortho_learner_model_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/dml/_rlearner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, Y, T, X, W, Z, nuisances, sample_weight, freq_weight, sample_var)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mY_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnuisances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         self._model_final.fit(X, T, T_res, Y_res, sample_weight=sample_weight,\n\u001b[0;32m---> 97\u001b[0;31m                               freq_weight=freq_weight, sample_var=sample_var)\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/dml/dml.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, T, T_res, Y_res, sample_weight, freq_weight, sample_var)\u001b[0m\n\u001b[1;32m    171\u001b[0m             filtered_kwargs = filter_none_kwargs(sample_weight=sample_weight,\n\u001b[1;32m    172\u001b[0m                                                  freq_weight=freq_weight, sample_var=target_var)\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfiltered_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This combination is not a feasible one!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/sklearn_extensions/model_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    259\u001b[0m                                        return_train_score=self.return_train_score)\n\u001b[1;32m    260\u001b[0m                           for estimator, param_grid in zip(self.estimator_list, self.param_grid_list)]\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_ind_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgcv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcv_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcv_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_ind_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcv_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_ind_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/econml/sklearn_extensions/model_selection.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m                                        return_train_score=self.return_train_score)\n\u001b[1;32m    260\u001b[0m                           for estimator, param_grid in zip(self.estimator_list, self.param_grid_list)]\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_ind_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgcv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcv_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcv_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_ind_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcv_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_ind_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/interpret/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'sample_weight'"
     ]
    }
   ],
   "source": [
    "learner_dic = generate_CATE_learners(model_y, model_t, X)\n",
    "ebm_learner_dic = generate_EBM_CATE_learners(model_y, model_t)\n",
    "\n",
    "records = run_and_report_mse(['Tlearner', 'NonParamDML'], learner_dic, Y, T, X, X_test, treatment_effect_test) \\\n",
    "    + run_and_report_mse(['EBM_Tlearner', 'EBM_NonParamDML'], ebm_learner_dic, Y, T, X, X_test, treatment_effect_test)\n",
    "# records = run_and_report_mse(learner_dic.keys(), learner_dic, Y, T, X, X_test, treatment_effect_test) + run_and_report_mse(ebm_learner_dic.keys(), ebm_learner_dic, Y, T, X, X_test, treatment_effect_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_df = pd.DataFrame.from_records(records)[['model_name', 'mse', 'error_50', 'error_95']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>mse</th>\n",
       "      <th>error_50</th>\n",
       "      <th>error_95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EBM_NonParamDML</td>\n",
       "      <td>1.779549</td>\n",
       "      <td>0.517346</td>\n",
       "      <td>2.092989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NonParamDML</td>\n",
       "      <td>1.945719</td>\n",
       "      <td>0.292297</td>\n",
       "      <td>2.189975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tlearner</td>\n",
       "      <td>1.983921</td>\n",
       "      <td>0.465572</td>\n",
       "      <td>2.225324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EBM_Tlearner</td>\n",
       "      <td>1.992881</td>\n",
       "      <td>0.296013</td>\n",
       "      <td>2.243950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model_name       mse  error_50  error_95\n",
       "3  EBM_NonParamDML  1.779549  0.517346  2.092989\n",
       "1      NonParamDML  1.945719  0.292297  2.189975\n",
       "0         Tlearner  1.983921  0.465572  2.225324\n",
       "2     EBM_Tlearner  1.992881  0.296013  2.243950"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_df.sort_values(by=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EBM CATE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/139882456186384/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/139882456186384/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(ebm_learner_dic['EBM_NonParamDML'].model_cate.explain_global())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ba1120f7315ec533dcbcb515e6944ded90bdb117ef81baf34be30f94735a085"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
